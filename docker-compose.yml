name: ai-toolkit

# All services start by default. Run: docker compose up -d
services:
  ollama:
    image: ${OLLAMA_IMAGE:-ollama/ollama:0.17.4}
    restart: unless-stopped
    environment:
      - OLLAMA_HOST=0.0.0.0:11434
    # Host port not exposed by default (backend-only). To expose for Cursor/direct use: -f docker-compose.ollama-expose.yml
    volumes:
      - ${DATA_PATH:-${BASE_PATH:-.}/data}/ollama:/root/.ollama
    healthcheck:
      # Image has no curl/wget; use ollama CLI to probe the server
      test: ["CMD", "ollama", "list"]
      start_period: 30s
      interval: 15s
      timeout: 10s
      retries: 5
    deploy:
      resources:
        limits:
          memory: 8G
    # GPU: add deploy via docker-compose.compute.yml (run scripts/detect_hardware.py)
    # Pin digest: OLLAMA_IMAGE=ollama/ollama:0.17.4@sha256:1edb4ab90ebbe34b484bb120ab8de22601f463834bfeca7f5a2de2ca6dad13ee
    networks:
      - backend

  model-gateway:
    build: ./model-gateway
    image: ai-toolkit-model-gateway:latest
    pull_policy: build
    restart: unless-stopped
    user: "1000:1000"
    read_only: true
    tmpfs:
      - /tmp
    cap_drop: [ALL]
    security_opt: [no-new-privileges:true]
    depends_on:
      ollama:
        condition: service_healthy
      dashboard:
        condition: service_started
    environment:
      - OLLAMA_URL=http://ollama:11434
      - VLLM_URL=${VLLM_URL:-}
      - DEFAULT_PROVIDER=ollama
      - DASHBOARD_URL=http://dashboard:8080
      - MODEL_CACHE_TTL_SEC=${MODEL_CACHE_TTL_SEC:-60}
    ports:
      - "${MODEL_GATEWAY_PORT:-11435}:11435"
    healthcheck:
      test: ["CMD", "python3", "-c", "import urllib.request; urllib.request.urlopen('http://localhost:11435/health')"]
      interval: 30s
      timeout: 10s
      retries: 3
    deploy:
      resources:
        limits:
          memory: 512M
    logging:
      driver: json-file
      options:
        max-size: "10m"
        max-file: "3"
    networks:
      - frontend
      - backend

  ops-controller:
    build: ./ops-controller
    image: ai-toolkit-ops-controller:latest
    pull_policy: build
    restart: unless-stopped
    cap_drop: [ALL]
    security_opt: [no-new-privileges:true]
    deploy:
      resources:
        limits:
          memory: 256M
    environment:
      - COMPOSE_PROJECT_DIR=/workspace
      - OPS_CONTROLLER_TOKEN=${OPS_CONTROLLER_TOKEN:-}
      - AUDIT_LOG_PATH=/data/audit.log
      - BASE_PATH=${BASE_PATH:-.}
      - DATA_PATH=${DATA_PATH:-${BASE_PATH:-.}/data}
      - COMPOSE_FILE=${COMPOSE_FILE:-docker-compose.yml}
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
      - ${BASE_PATH:-.}:/workspace:ro
      - ${DATA_PATH:-${BASE_PATH:-.}/data}/ops-controller:/data
    healthcheck:
      test: ["CMD", "python3", "-c", "import urllib.request; urllib.request.urlopen('http://localhost:9000/health')"]
      interval: 30s
      timeout: 10s
      retries: 3
    logging:
      driver: json-file
      options:
        max-size: "10m"
        max-file: "3"
    # No host port - dashboard calls internally. Add "9000:9000" for debugging.
    networks:
      - backend

  dashboard:
    build: ./dashboard
    restart: unless-stopped
    user: "1000:1000"
    read_only: true
    tmpfs:
      - /tmp
    cap_drop: [ALL]
    security_opt: [no-new-privileges:true]
    depends_on:
      ollama:
        condition: service_healthy
    ports:
      - "8080:8080"
    deploy:
      resources:
        limits:
          memory: 256M
    extra_hosts:
      - "host.docker.internal:host-gateway"
    healthcheck:
      test: ["CMD", "python3", "-c", "import urllib.request; urllib.request.urlopen('http://localhost:8080/api/health')"]
      interval: 30s
      timeout: 10s
      retries: 3
    logging:
      driver: json-file
      options:
        max-size: "10m"
        max-file: "3"
    environment:
      - OLLAMA_URL=http://ollama:11434
      - MODELS_DIR=/models
      - SCRIPTS_DIR=/scripts
      - MCP_CONFIG_PATH=/mcp-config/servers.txt
      - MCP_GATEWAY_URL=http://mcp-gateway:8811
      - MCP_GATEWAY_SERVERS=${MCP_GATEWAY_SERVERS:-duckduckgo}
      - OPS_CONTROLLER_URL=http://ops-controller:9000
      - OPS_CONTROLLER_TOKEN=${OPS_CONTROLLER_TOKEN:-}
      - DASHBOARD_AUTH_TOKEN=${DASHBOARD_AUTH_TOKEN:-}
      - DASHBOARD_PASSWORD=${DASHBOARD_PASSWORD:-}
    volumes:
      - ${BASE_PATH:-.}/models/comfyui:/models
      - ${BASE_PATH:-.}/scripts:/scripts:ro
      - ${DATA_PATH:-${BASE_PATH:-.}/data}/mcp:/mcp-config
    networks:
      - frontend
      - backend

  open-webui:
    image: ${OPEN_WEBUI_IMAGE:-ghcr.io/open-webui/open-webui:v0.8.4}
    restart: unless-stopped
    depends_on:
      - ollama
      - model-gateway
    ports:
      - "3000:8080"
    deploy:
      resources:
        limits:
          memory: 1G
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/"]
      interval: 30s
      timeout: 10s
      retries: 3
    environment:
      # Use model gateway for OpenAI-compatible API (recommended), or Ollama directly
      - OLLAMA_BASE_URL=${OLLAMA_BASE_URL:-http://ollama:11434}
      - OPENAI_API_BASE=${OPENAI_API_BASE:-http://model-gateway:11435/v1}
      # Auth: True by default. Set WEBUI_AUTH=False in .env for single-user local use.
      - WEBUI_AUTH=${WEBUI_AUTH:-True}
    volumes:
      - ${DATA_PATH:-${BASE_PATH:-.}/data}/open-webui:/app/backend/data
    networks:
      - frontend

  model-puller:
    profiles: [models]
    image: curlimages/curl:8.10.1
    depends_on:
      - ollama
    restart: "no"
    environment:
      - OLLAMA_URL=http://ollama:11434
      - MODELS=${MODELS:-deepseek-r1:7b,deepseek-coder:6.7b,nomic-embed-text}
    entrypoint: ["/bin/sh", "-c"]
    command:
      - |
        set -eu
        echo "Waiting for Ollama ..."
        until curl -fsS "$$OLLAMA_URL/api/version" >/dev/null; do sleep 2; done
        echo "Pulling: $$MODELS"
        IFS=','; for m in $$MODELS; do
          m="$$(echo "$$m" | tr -d ' ')"
          [ -z "$$m" ] && continue
          echo "==> $$m"
          curl -fsS "$$OLLAMA_URL/api/pull" \
            -H "Content-Type: application/json" \
            -d "{\"model\":\"$$m\",\"stream\":false}"
        done
        echo "Done."
    networks:
      - backend

  comfyui-model-puller:
    profiles: [comfyui-models]
    image: python:3.12.8-slim
    restart: "no"
    environment:
      - MODELS_DIR=/models
    volumes:
      - ${BASE_PATH:-.}/models/comfyui:/models
      - ${BASE_PATH:-.}/scripts:/scripts:ro
    command: ["python3", "/scripts/comfyui/pull_comfyui_models.py"]
    networks:
      - frontend

  comfyui:
    image: ${COMFYUI_IMAGE:-yanwk/comfyui-boot:cpu}
    container_name: comfyui
    restart: unless-stopped
    deploy:
      resources:
        limits:
          memory: 4G
    # ComfyUI: run scripts/detect_hardware.py to auto-configure GPU (NVIDIA/AMD/Intel) or CPU
    ports:
      - "8188:8188"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8188/"]
      interval: 30s
      timeout: 10s
      retries: 3
    logging:
      driver: json-file
      options:
        max-size: "10m"
        max-file: "3"
    environment:
      - CLI_ARGS=--cpu
      - PYTORCH_CUDA_ALLOC_CONF=
    volumes:
      - ${BASE_PATH:-.}/data/comfyui-storage:/root
      - ${BASE_PATH:-.}/models/comfyui:/root/ComfyUI/models
      - ${DATA_PATH:-${BASE_PATH:-.}/data}/comfyui-output:/root/ComfyUI/output
    networks:
      - frontend

  n8n:
    image: ${N8N_IMAGE:-docker.n8n.io/n8nio/n8n}
    restart: unless-stopped
    # Run as non-root (n8n image uses node user; 1000:1000 matches typical node uid)
    user: "1000:1000"
    depends_on:
      - mcp-gateway
    ports:
      - "5678:5678"
    deploy:
      resources:
        limits:
          memory: 1G
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:5678/"]
      interval: 30s
      timeout: 10s
      retries: 3
    logging:
      driver: json-file
      options:
        max-size: "10m"
        max-file: "3"
    environment:
      - N8N_HOST=0.0.0.0
      - N8N_PORT=5678
    volumes:
      - ${DATA_PATH:-${BASE_PATH:-.}/data}/n8n-data:/home/node/.n8n
      - ${DATA_PATH:-${BASE_PATH:-.}/data}/n8n-files:/files
    networks:
      - frontend

  mcp-gateway:
    build: ./mcp
    image: ai-toolkit-mcp-gateway:latest
    pull_policy: build
    container_name: mcp-gateway
    restart: unless-stopped
    deploy:
      resources:
        limits:
          memory: 512M
    environment:
      - MCP_CONFIG_FILE=/mcp-config/servers.txt
      - MCP_GATEWAY_PORT=8811
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
      - ${DATA_PATH:-${BASE_PATH:-.}/data}/mcp:/mcp-config
    ports:
      - "${MCP_GATEWAY_PORT:-8811}:8811"
    healthcheck:
      test: ["CMD", "wget", "-q", "-O", "/dev/null", "http://localhost:8811/mcp"]
      interval: 30s
      timeout: 10s
      retries: 3
    logging:
      driver: json-file
      options:
        max-size: "10m"
        max-file: "3"
    # Optional: mount secrets for MCPs that need API keys (e.g. github-official, brave)
    # secrets: [mcp_secrets]
    # Uncomment and create mcp/.env with GITHUB_PERSONAL_ACCESS_TOKEN etc.
    networks:
      - frontend

  openclaw-workspace-sync:
    image: alpine:3
    restart: "no"
    volumes:
      - ${BASE_PATH:-.}/openclaw/workspace:/templates:ro
      - ${OPENCLAW_WORKSPACE_DIR:-${BASE_PATH:-.}/data/openclaw/workspace}:/workspace
    entrypoint: ["/bin/sh", "-c", "cp -f /templates/SOUL.md /templates/AGENTS.md /templates/TOOLS.md /workspace/ 2>/dev/null || true"]
    networks:
      - frontend

  openclaw-config-sync:
    image: python:3.12-alpine
    restart: "no"
    env_file:
      - .env
    environment:
      - OPENCLAW_GATEWAY_TOKEN=${OPENCLAW_GATEWAY_TOKEN:-}
    volumes:
      - ${OPENCLAW_CONFIG_DIR:-${BASE_PATH:-.}/data/openclaw}:/config
      - ${BASE_PATH:-.}/openclaw/scripts:/scripts:ro
    entrypoint: ["python3", "/scripts/merge_gateway_config.py"]
    networks:
      - frontend

  # Install MCP bridge plugin into mounted config (image entrypoint runs openclaw correctly).
  openclaw-plugin-install:
    image: ${OPENCLAW_IMAGE:-ghcr.io/phioranex/openclaw-docker:latest}
    restart: "no"
    depends_on:
      openclaw-config-sync:
        condition: service_completed_successfully
    environment:
      HOME: /home/node
    volumes:
      - ${OPENCLAW_CONFIG_DIR:-${BASE_PATH:-.}/data/openclaw}:/home/node/.openclaw
    command: ["plugins", "install", "openclaw-mcp-bridge"]
    networks:
      - frontend

  # Add MCP plugin config to openclaw.json after plugin-install (config-sync removes it
  # so plugin-install can run; this step adds it back for the gateway).
  openclaw-plugin-config:
    image: python:3.12-alpine
    restart: "no"
    depends_on:
      openclaw-plugin-install:
        condition: service_completed_successfully
    volumes:
      - ${OPENCLAW_CONFIG_DIR:-${BASE_PATH:-.}/data/openclaw}:/config
      - ${BASE_PATH:-.}/openclaw/scripts:/scripts:ro
    entrypoint: ["python3", "/scripts/add_mcp_plugin_config.py"]
    networks:
      - frontend

  openclaw-gateway:
    image: ${OPENCLAW_IMAGE:-ghcr.io/phioranex/openclaw-docker:latest}
    container_name: openclaw-gateway
    depends_on:
      ollama:
        condition: service_started
      model-gateway:
        condition: service_started
      mcp-gateway:
        condition: service_started
      openclaw-workspace-sync:
        condition: service_completed_successfully
      openclaw-config-sync:
        condition: service_completed_successfully
    env_file:
      - .env
    environment:
      HOME: /home/node
      TERM: xterm-256color
      NODE_ENV: production
      OPENCLAW_SKIP_SERVICE_CHECK: "true"
      OPENCLAW_GATEWAY_BIND: "lan"
      # Ollama: expose local models to OpenClaw (native API, no /v1)
      OLLAMA_API_KEY: "ollama-local"
      OLLAMA_BASE_URL: "http://ollama:11434"
    volumes:
      - ${OPENCLAW_CONFIG_DIR:-${BASE_PATH:-.}/data/openclaw}:/home/node/.openclaw
      - ${OPENCLAW_WORKSPACE_DIR:-${BASE_PATH:-.}/data/openclaw/workspace}:/home/node/.openclaw/workspace
    ports:
      - "${OPENCLAW_GATEWAY_PORT:-18789}:18789"
      - "${OPENCLAW_BRIDGE_PORT:-18790}:18790"
    stdin_open: true
    tty: true
    init: true
    restart: unless-stopped
    deploy:
      resources:
        limits:
          memory: 2G
    healthcheck:
      test: ["CMD", "wget", "-q", "-O", "/dev/null", "http://localhost:18789"]
      start_period: 60s
      interval: 30s
      timeout: 10s
      retries: 3
    logging:
      driver: json-file
      options:
        max-size: "10m"
        max-file: "3"
    command: ["gateway", "--allow-unconfigured"]
    networks:
      - frontend
      - backend

  openclaw-cli:
    profiles: [openclaw-cli]
    image: ${OPENCLAW_IMAGE:-ghcr.io/phioranex/openclaw-docker:latest}
    depends_on:
      - ollama
      - openclaw-gateway
    env_file:
      - .env
    environment:
      HOME: /home/node
      TERM: xterm-256color
      NODE_ENV: production
      BROWSER: echo
      OLLAMA_API_KEY: "ollama-local"
      OLLAMA_BASE_URL: "http://ollama:11434"
    volumes:
      - ${OPENCLAW_CONFIG_DIR:-${BASE_PATH:-.}/data/openclaw}:/home/node/.openclaw
      - ${OPENCLAW_WORKSPACE_DIR:-${BASE_PATH:-.}/data/openclaw/workspace}:/home/node/.openclaw/workspace
    stdin_open: true
    tty: true
    init: true
    entrypoint: ["node", "/app/dist/index.js"]
    networks:
      - frontend

networks:
  frontend:
    name: ai-toolkit-frontend
  backend:
    name: ai-toolkit-backend
    # Backend-only containers (ollama, ops-controller) have no outbound internet.
    # If DNS or healthchecks fail, set internal: false or see docs/runbooks/SECURITY_HARDENING.md
    internal: true
