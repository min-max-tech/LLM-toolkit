services:
  ollama:
    image: ollama/ollama:latest
    restart: unless-stopped
    environment:
      - OLLAMA_HOST=0.0.0.0:11434
    ports:
      - "11434:11434" # remove if you ONLY want access via Open WebUI
    volumes:
      - ollama:/root/.ollama
    # --- NVIDIA GPU (optional) ---
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]

  open-webui:
    image: ghcr.io/open-webui/open-webui:main
    restart: unless-stopped
    depends_on:
      - ollama
    ports:
      - "3000:8080"
    environment:
      - OLLAMA_BASE_URL=http://ollama:11434
      - WEBUI_AUTH=False
    volumes:
      - open-webui:/app/backend/data

  model-puller:
    image: curlimages/curl:latest
    depends_on:
      - ollama
    restart: "no"
    environment:
      - OLLAMA_URL=http://ollama:11434
      # Set in .env or here. Comma-separated; good defaults below.
      - MODELS=${MODELS:-deepseek-r1:7b,deepseek-coder:6.7b,nomic-embed-text}
    entrypoint: ["/bin/sh", "-c"]
    command: |
      set -eu
      echo "Waiting for Ollama at $$OLLAMA_URL ..."
      until curl -fsS "$$OLLAMA_URL/api/version" >/dev/null; do sleep 2; done

      echo "Pulling models: $$MODELS"
      IFS=','; for m in $$MODELS; do
        m="$$(echo "$$m" | tr -d ' ')"
        [ -z "$$m" ] && continue
        echo "==> Pulling $$m"
        curl -fsS "$$OLLAMA_URL/api/pull" \
          -H "Content-Type: application/json" \
          -d "{\"model\":\"$$m\",\"stream\":false}"
      done

      echo "Done."

  comfyui:
    image: ghcr.io/lecode-official/comfyui-docker:latest
    container_name: comfyui
    restart: unless-stopped
    ports:
      - "8188:8188"
    environment:
      USER_ID: "${USER_ID:-1000}"
      GROUP_ID: "${GROUP_ID:-1000}"
    volumes:
      - comfyui-models:/opt/comfyui/models
      - comfyui-custom-nodes:/opt/comfyui/custom_nodes
      - comfyui-output:/opt/comfyui/output
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]

  n8n:
    image: docker.n8n.io/n8nio/n8n
    restart: unless-stopped
    ports:
      - "5678:5678"
    environment:
      - N8N_HOST=0.0.0.0
      - N8N_PORT=5678
    volumes:
      - n8n-data:/home/node/.n8n
      - n8n-files:/files

volumes:
  ollama:
  open-webui:
  comfyui-models:
  comfyui-custom-nodes:
  comfyui-output:
  n8n-data:
  n8n-files:
