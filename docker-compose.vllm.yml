# Optional vLLM service â€” use with: docker compose -f docker-compose.yml -f docker-compose.vllm.yml --profile vllm up -d
# Then set VLLM_URL=http://vllm:8000 for model-gateway (e.g. in .env) so /v1/models and chat route to vLLM.
# Model gateway routes requests with model prefix "vllm/<name>" to this service.
name: ai-toolkit

services:
  vllm:
    profiles: [vllm]
    # Pinned digest for supply-chain safety. Override with VLLM_IMAGE for a different tag/digest.
    image: ${VLLM_IMAGE:-vllm/vllm-openai@sha256:89523c8293bc02a4dfaaa80079a5347dc3952464a33a501d5de329921eea7ec7}
    restart: unless-stopped
    environment:
      - MODEL=${VLLM_MODEL:-meta-llama/Llama-3.2-3B-Instruct}
    deploy:
      resources:
        limits:
          memory: 16G
    # No host port by default; model-gateway reaches http://vllm:8000 on backend network.
    # Expose for direct access: ports: ["8000:8000"]
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      start_period: 120s
      interval: 30s
      timeout: 10s
      retries: 3
    logging:
      driver: json-file
      options:
        max-size: "10m"
        max-file: "3"
    networks:
      - backend
